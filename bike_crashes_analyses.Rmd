---
title: "Bike Crash Analyses"
author: "Elliott O'Brien"
date: "`R Sys.time()"
output: pdf_document
---

```{r setup, include = FALSE}
library('knitr')
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_libraries, include = FALSE}
library('tidyverse')
theme_set(theme_bw())
library('rnaturalearth')
library('rnaturalearthdata')

# Machine-learning and stats packages
library('caret')
library('caretEnsemble')
library('ranger')
library('doParallel')

# default train control object
my_default_control <- trainControl(
  summaryFunction = multiClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE
)
```

# Data Preparation (separate file)

```{r}
rmarkdown::render('data_exploration.Rmd')
```

# Pre-Processing and Model Setup

Using the caret package, a pre-processing step will be used to remove near-zero
variance variables from the data set, to impute missing data, and to center and 
scale variables.  The trainControl object will be used to control the type of 
cross validation used to decrease overfitting.

Pre-Processing to be carried out:
* remove near-zero variance
* implement imputation (median, knn) for missing data
* center and scale to normalize data
* attempt principal components analysis (PCA) for feature reduction

As it can be seen in the table below, KNN does the best job imputing for various 
variables with missing data.

```{r pre_process}

# load cleaned-tidied data
bike_crashes_clean <- readRDS('./derived_data/bike_crashes_clean.rds')
bike_crash_dummies <- readRDS('./derived_data/bike_crash_dummies.rds')

########################
# Check Missing Values #
########################

# percent of missing data in each column
bike_crash_dummies %>%
  map(~ round(sum(is.na(.x)) / nrow(bike_crash_dummies) * 100, 1)) %>%
  as_tibble() %>%
  select(where(~.x > 10)) %>%
  kable(caption = 'Variables with more than 10% missing values')

################################################
# Split Data into training and testing dataset #
################################################
set.seed(24)

train_index <- createDataPartition(bike_crash_dummies$BikeInjuryDichot, p = .8, list=FALSE)
train_dat <- bike_crash_dummies[train_index,]
test_dat <- bike_crash_dummies[-train_index,]

# percent of missing data in each column
train_dat %>%
  map(~ round(sum(is.na(.x)) / nrow(train_dat) * 100, 1)) %>%
  as_tibble() %>%
  select(where(~.x > 10)) %>%
  kable(caption = 'training dataset vars with more than 10% missing')

test_dat %>%
  map(~ round(sum(is.na(.x)) / nrow(test_dat) * 100, 1)) %>%
  as_tibble() %>%
  select(where(~.x > 10)) %>%
  kable(caption = 'test dataset vars with more than 10% missing')

##################
# Pre-Processing #
##################

# attempt some imputations if possible
preproc_mdl <- preProcess(
  train_dat %>%
    select(-BikeInjuryDichot) %>% 
    as.data.frame,
  # centering and scaling done by knnImpute
  method = c(
    'nzv'#, 
    # 'knnImpute'
  )
)

preproc_mdl
# preproc_mdl$method

# run the preProcess predictions to create new dataset
bike_crashes_preProcessed <- predict(
  preproc_mdl,
  train_dat %>% as.data.frame
)
bike_crashes_preProcessed %>% glimpse()

# convert the BikeInjuryDichot variable from numeric to a factor that is
# variable name friendly
bike_crashes_preProcessed <-
  bike_crashes_preProcessed %>%
    mutate(
      BikeInjuryDichot = factor(
        if_else(BikeInjuryDichot == 1, 'Serious', 'Non.Serious'), 
        levels = c('Serious', 'Non.Serious')
      )
    )

test_dat <-
  test_dat %>%
    mutate(
      BikeInjuryDichot = factor(
        if_else(BikeInjuryDichot == 1, 'Serious', 'Non.Serious'), 
        levels = c('Serious', 'Non.Serious')
      )
    )

########################
# Check Missing Values #
########################

# percent of missing data in each column with > 10% missing
bike_crashes_preProcessed %>%
  map(~ round(sum(is.na(.x)) / nrow(bike_crashes_preProcessed) * 100, 1)) %>%
  as_tibble() %>%
  select(where(~.x > 10)) %>%
  kable(caption = 'Percent Missing Values (w/ more than 10% missing)')

# Serious Injuries are rare events, consider using down/up sampling
bike_crashes_preProcessed %>% 
  count(BikeInjuryDichot) %>% 
  mutate(pct = n / sum(n) * 100) %>%
  kable(caption = 'Preprocessed training data class summary')

test_dat %>% 
  count(BikeInjuryDichot) %>% 
  mutate(pct = n / sum(n) * 100) %>%
  kable(caption = 'Test data data class summary')
```

## Modeling

### Linear Classifiers 

```{r}
# caret control
binomial_control <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = 'final',
  sampling = 'up',
  method = 'repeatedcv',
  repeats = 3
)

set.seed(42)


######################################
# Elasticnet Logistic Classification #
######################################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(7)
registerDoParallel(cl)

binom_mdl <- train(
  BikeInjuryDichot ~ .,
  data = bike_crashes_preProcessed,
  method = 'glmnet',
  family = 'binomial',
  na.action = na.omit,
  trControl = binomial_control,
  tuneLenth = 20
)
stopCluster(cl)
b <- Sys.time()
binom_time <- b - a
print(binom_time)

#################################################################
# Elasticnet Logistic Classification using Principal Components #
#################################################################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(7)
registerDoParallel(cl)
binom_PCA_mdl <- train(
  BikeInjuryDichot ~ .,
  data = bike_crashes_preProcessed,
  method = 'glmnet',
  family = 'binomial',
  preProcess = c('pca'),
  na.action = na.omit,
  trControl = binomial_control,
  tuneLength = 20
)
stopCluster(cl)
b <- Sys.time()
binom_PCA_time <- b - a
print(binom_PCA_time)
```

### Non-linear Classifcation Models (Random Forests)

```{r}
set.seed(42)

###############
# Naive Bayes #
###############
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(7)
registerDoParallel(cl)
nb_mdl <- train(
  BikeInjuryDichot ~ .,
  data = bike_crashes_preProcessed,
  method = 'naive_bayes',
  na.action = na.omit,
  trControl = binomial_control
)
stopCluster(cl)
b <- Sys.time()
nb_time <- b - a
print(nb_time)

#################
# Random Forest #
#################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(7)
registerDoParallel(cl)
rf_mdl <- train(
  BikeInjuryDichot ~ .,
  data = bike_crashes_preProcessed,
  method = 'ranger',
  na.action = na.omit,
  trControl = binomial_control
)
stopCluster(cl)
b <- Sys.time()
rf_time <- b - a
print(rf_time)
```

### Emsemble Classifier

Logistic Regression Naive Bayes will be used in an ensemble learning model.
Note that random forests took the longest to run with no improvement in accuracy
and is not used in the ensemble reason for these reasons.

```{r}
set.seed(42)

# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(7)
registerDoParallel(cl)
model_list <- caretList(
  BikeInjuryDichot ~ .,
  data = bike_crashes_preProcessed %>%
    filter(complete.cases(.)),
  tuneList = list(
    binom_mdl = caretModelSpec(
      method = 'glmnet',
      family = 'binomial',
      tuneLength = 20
    ),
    nb_mdl = caretModelSpec(method = 'naive_bayes')
  ),
  trControl = trainControl(
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    savePredictions = 'final',
    sampling = 'up',
    method = 'repeatedcv',
    repeats = 3
  )
)
stopCluster(cl)

cl <- makeCluster(7)
registerDoParallel(cl)
ensemble_mdl <- caretEnsemble(
  model_list, 
  metric = 'ROC',
  trControl = trainControl(
    summaryFunction = twoClassSummary,
    classProbs = TRUE
  )
)
stopCluster(cl)
b <- Sys.time()
ensemble_time <- b - a
print(ensemble_time)

ensemble_mdl
ggplot(resamples(model_list))
modelCor(resamples(model_list))
```

## Predictions

```{r}
# performance function
mdl_performance <- function(mdl, conf_matrix = FALSE, ensemble = FALSE, mdl_lst = NULL) {
  
  mdl_predictors <- NULL
  mdl_test_dat <- NULL
  if(ensemble) {
    mdl_predictors <- map(mdl_lst, predictors) %>% flatten() %>% unlist() %>% unique()
    mdl_test_dat <- test_dat[complete.cases(test_dat[, names(test_dat) %in% mdl_predictors]),]$BikeInjuryDichot
  } else {
    mdl_predictors <- attr(formula(mdl), 'term.labels')
    mdl_test_dat <- test_dat[complete.cases(test_dat[, names(test_dat) %in% mdl_predictors]),]$BikeInjuryDichot
  }
    
  mdl_pred <- predict(mdl, newdata = test_dat)

  conf_mat <- NULL
  if(conf_matrix) {
    conf_mat <- confusionMatrix(
      factor(mdl_pred, levels = c('Serious', 'Non.Serious')),
      factor(mdl_test_dat, levels = c('Serious', 'Non.Serious'))
    )
  }
  
  mdl_pred_prob <- predict(mdl, newdata = test_dat, type = 'prob')
  if(ensemble) {
    mdl_pred_prob <- tibble(
      Serious = mdl_pred_prob
    ) %>%
    mutate(
      Non.Serious = 1 - Serious
    ) 
  }

  perf <- tibble(
      p = seq(.01, 0.99, by = .01),
      mdl_prob = list(mdl_pred_prob),
      obs = list(mdl_test_dat)
    ) %>%
    mutate(
      pred = map2(
        p, 
        mdl_prob,
        function(p, probs) {
          if_else(probs$Serious > p, 'Serious', 'Non.Serious')
        }
      )
    ) %>% 
      unnest(c(obs, pred)) %>%
      group_by(p) %>%
      summarize(
        n_TP = sum(obs == 'Serious' & pred == obs),
        n_obs_P = sum(obs == 'Serious'),
        sensitivity = n_TP / n_obs_P,
        n_TN = sum(obs == 'Non.Serious' & pred == obs),
        n_obs_N = sum(obs == 'Non.Serious'),
        specificity = n_TN / n_obs_N,
        accuracy = (n_TP + n_TN) / (n_obs_P + n_obs_N)
      ) %>%
      select(-matches('obs'))
  
  perf_plot <- perf %>%
    ggplot(data = .) +
    geom_line(aes(x = p, y = sensitivity, color = 'sensitivity'), size = 1) +
    geom_line(aes(x = p, y = specificity, color = 'specificity'), size = 1) +
    geom_line(aes(x = p, y = accuracy, color = 'accuracy'), size = 1) +
    xlab('p (percent cutoff)') +
    ylab('performance rate') +
    scale_color_manual(
      name = 'Performance Measures', 
      values = c('sensitivity' = 'red', 'specificity' = 'dark green', 'accuracy' = 'blue'),
      ) +
    scale_x_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = seq(0, 1, by = 0.05))
  
  return(list(
    mdl_probs = mdl_pred_prob,
    performance_plot = perf_plot,
    performance_tbl = perf,
    confidence_matrix = conf_mat
  ))
}
```

### Linear Predictions

```{r}
# Logistic Classification
mdl_performance(binom_mdl, conf_matrix = TRUE)

# Logistic Classification with PCA 
mdl_performance(binom_PCA_mdl, conf_matrix = TRUE)
```

### Non-linear Predictions

```{r}
# Naive Bayes
mdl_performance(nb_mdl, conf_matrix = TRUE)

# Random Forest 
mdl_performance(rf_mdl, conf_matrix = TRUE)
```

### Ensemble Predictions

```{r}
mdl_performance(ensemble_mdl, ensemble = TRUE, mdl_lst = model_list)
```
