---
title: "Bike Crash Analyses"
author: "Elliott O'Brien"
date: '`r Sys.time()`'
output:

  pdf_document: 
    toc: yes
    fig_caption: yes
    number_sections: yes
---

```{r setup, include = FALSE}
library('knitr')
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r load_libraries, message=FALSE, warning=FALSE, include=FALSE}
library('tidyverse')
theme_set(theme_bw())
library('rnaturalearth')
library('rnaturalearthdata')

# Machine-learning and stats packages
library('caret')
library('caretEnsemble')
library('ranger')
library('doParallel')

# default train control object
my_default_control <- trainControl(
  summaryFunction = multiClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE
)
```

# Background and Motivation

Assuming I work for the state of North Carolina as a data scientist, I'm
working on a project that will investigate bicycle safety in the state of 
North Carolina in a effort to understand what factors to consider in decreasing
the number serious bicycle injuries as caused by bicycle-vehicle collisions.

In this report, I will be exploring machine learning models with the caret 
package in R and attempt to find the best model or ensemble model that can 
predict a bike injury being "serious" vs. "non-serious". The resulting model
will be used to find the road, bicyclist, and driver features that most affect
the seriousness of an injury.

# Analytical Dataset

```{r message=FALSE, warning=FALSE, include=FALSE}
datacamp_dataset_repo <- 'https://raw.githubusercontent.com/datacamp/careerhub-data/master'
bike_crashes <- read_csv(
  str_c(
    datacamp_dataset_repo, 
    '/Pedestrian%20and%20Bike%20Crashes/pedestrian_bike_crash.csv'
  )
)
```

## Data Description

The large number of variables (61) with multiple levels for some categorical 
variables. 

```{r echo=FALSE}
bike_crashes %>% 
  map_df(class) %>% 
  pivot_longer(
    -c(OBJECTID_1), 
    values_to = 'Data Type', 
    names_to = 'Variable'
  ) %>%
  mutate(var_n = row_number()) %>%
  select(-OBJECTID_1) %>%
  select(var_n, everything()) %>% 
  kable(caption = 'Dataset Feature Discription')
```

The bike crashes dataset is sourced from the Department of
Transportation in the state North Carolina from the years April 2007 -
September 2019 ([North Carolina Bicycle and Pedestrian Crash Data Tool
(pedbikeinfo.org)](https://www.pedbikeinfo.org/pbcat_nc/)). Raw data
consists of 62 columns which are a mixture of numerical and character
data types. The crash severity variable, CrashSevr, will be used as a
response variable in a binary classification model, "Serious" vs.
"Non-serious". Currently the variable is categorical with multiple
levels of severity:

```{r}
bike_crashes %>%
  count(BikeInjury) %>%
  mutate(pct = n / sum(n) * 100) %>%
  kable(caption = 'Raw bike injury class values', digits = 2)
```

However, since we are only interested in reducing severe injuries, the 5
categories above will be re-binned into 'Serious' and 'Non-Serious' injuries as 
follows:

-   serious injury = {suspected serious injury OR killed}

-   non-serious injury = {no injury OR possible injury OR suspected
    minor injury}

and assigned to a new variable BikeInjurySerious which will be our
response variable in our models.  Below is a summary of the prevalence of 
serious injuries to bicyclists.  As it can be seen the classes are unbalanced
with only 7.44% of the observations resulting in serious injuries to the
bicyclist.  This unbalance will be talked about in more detail in the 
performance metrics section below.

```{r}
bike_crashes %>% 
  mutate(
    BikeInjurySerious = if_else(str_detect(BikeInjury, 'A: |K: '), 'Yes', 'No')
  ) %>% 
  count(BikeInjurySerious) %>%
  mutate(
    pct = n / sum(n) * 100
  ) %>%
  kable(caption = 'Prevalence of Serious Bike Accidents', digits = 2)
```

# Performance Metrics

Due to the imbalance of the target classes, 7.44% are serious injuries and 
92.56% non-serious, the performance metric accuracy will not be useful in
selecting the best model.  ROC area under the curve (AUC) is a more useful
metric as it attempts to balance the true positive rate (sensitivity) and the
false positive rate by optimizing the area under the curve created by graphing
false positive rate vs. true positive rate.

After training the models on training data, optimum probability cut-off 
values will be explored using model predictions. The best probability cut-off 
will be determined based on the assumption that the cost of a false negative,
predict non-serious injury when in-fact was a serious injury, is higher
than a false positive, predict serious injury when in-fact was
non-serious injury. The high cost for a false negative is higher because
lives could potentially be lost or a life-changing injury, while the
false positive only results in potentially spending more money on road
safety for bicyclists which pays off in the long run anyways. Therefore,
probability cut-offs will be chosen to optimize sensitivity (true
positive rate) along with accuracy, with specificity (false negative
rate) being the last priority. With this in mind, let's assume that we
want to have as close to 75% for sensitivity. Specificity can be relaxed as 
low as 50%. The models that don't meet these criteria will not be included in an 
ensemble model.

# Data Exploration

## Data Prep and Cleaning

data cleaning is done in this section using tidyverse methods.

### Data Issues

There are many issues with the data set not being tidy and clean. Some variables will need to be converted to
a numerical datatype for the machine learning models to work properly. Categorical character columns will
need to be converted to factors. Dummy variables will be created for every column that is not numerical. This
will likely increase the number of features in the dataset to the scale of hundreds of features. Feature reduction
will be carried out to reduce memory usage and computation time of fitting models. Feature reduction will
be carried out by removing features that have little to no variability and will thus not contribute much
information to the models being used. Additionally, features that have high correlation with other features
will be removed to improve model performance.

Some observations in the dataset that are missing data and these values have been imputed manually. For
numerical missing data such as age missing values were imputed based on medians of age groups if age
group variable is available, otherwise age was imputed using the median age of the full dataset based on
the variable in question. For categorical variables, after being converted to dummy variables (i.e. 0 or 1 in
value), missing values were imputed with a 0 to help in achieving a complete dataset as possible.

#### Unneccessary variables

The following variables are obviously copies of other variables.

*   X and Y are longitude and latitude which are already present in the data
*   OBJECTID and OBJECTID_1 seem like a row number id which is not a very useful 
feature.  Also these are duplicated columns.

```{r clean_data, echo=TRUE, message=FALSE, warning=FALSE}
bike_crashes <- bike_crashes %>%
  select(-X, -Y, -matches('OBJECT'))
```

*Deaths and bicyclst killed descrepency*

```{r}
bike_crashes %>%
  summarize(
    total_deaths = sum(str_detect(CrashSevr, 'K:')),
    total_bike_killed = sum(str_detect(BikeInjury, 'K:')),
    total_drvr_killed = sum(str_detect(DrvrInjury, 'K:'))
  ) %>%
  mutate(
    total_bike_drvr_killed = total_bike_killed + total_drvr_killed
  ) %>%
  kable(caption = 'Death count discrepency of dataset')
```

278 total deaths, 7 deaths that aren't clear since there is no indication if
bicyclist or driver was killed.  271 deaths that were biking or driving, mostly 
bicyclist (269) deaths. There are only 2 driver deaths in the whole dataset.

These are the 7 deaths that can't be explained by bicyclist(s) and
driver(s). There seems to be a third party involved that is not listed in
records, i.e. other pedestrian or other cyclist that didn't cause the crash
but was affected by the crash.

Since we are most concerned with serious bicycle injuries, these records will be
leaved as is since no serious injury to the bicyclist was recorded.  However, it
the CrashSevr and DrvrInjury variables will be removed since we already have 
BikeInjury variable.

```{r}
bike_crashes <- bike_crashes %>%
  select(-DrvrInjury, -CrashSevr)
```

*Duplicates*

Note duplicate crash report for CrashID == 1041566349; note that only
CrashLoc and CrashType are different. Based on the first record, the driver is
taking a right turn, but listed as non-intersection; in second record,
CrashType is "motorist overtaking - Other/Unknown" and Crash Location is
listed as intersection. Therefore, the CrashLoc was likely an intersection
where a motorist overtook a bicyclist.  This crash record will need to be
fixed to reflect this and remove duplication.

```{r}
bike_crashes[duplicated(bike_crashes$CrashID), 'CrashID'] %>% 
  kable(caption = 'Duplicate Record CrashID')
bike_crashes %>%
  filter(CrashID == 104156349) %>%
  glimpse()
```

```{r}
# Fix duplicate record
bike_crashes <- 
  bike_crashes %>%
  filter(CrashID != '104156349') %>%
  bind_rows(
    bike_crashes %>% 
      filter(CrashID == '104156349') %>%
      slice(1) %>% # grab first record and make changes
      mutate(
        CrashLoc = 'Intersection-Related',
        CrashType = 'Motorist Right Turn - Same Direction',
        CrashGrp = 'Motorist Right Turn / Merge'
      )
  )
```

### General data preparation

Some observations in the dataset that are missing data and these values
have been imputed manually. For numerical missing data such as age
missing values were imputed based on medians of age groups if age group
variable is available, otherwise age was imputed using the median age of
the full dataset based on the variable in question. For categorical
variables, after being converted to dummy variables (i.e. 0 or 1 in
value), missing values were imputed with a 0 to help in achieving a
complete dataset as possible.

```{r}
#################
# Data Cleaning #
#################
bike_crashes_clean <- 
  bike_crashes %>%
  mutate(
    AmbulanceR = fct_relevel(AmbulanceR, 'No', 'Yes'),
    BikeAge = as.numeric(BikeAge),
    # unlikely anyone over 100 years old
    BikeAge = if_else(BikeAge > 100 | BikeAge == 'Unknown', NA_real_, BikeAge),
    BikeAgeGrp = case_when(
      BikeAgeGrp == '06-Oct' ~ '6-10',
      BikeAgeGrp == 'Nov-15' ~ '11-15',
      BikeAgeGrp == 'Unknown' ~ NA_character_,
      TRUE ~ BikeAgeGrp
    ),
    BikeAlcDrg = case_when(
      BikeAlcDrg == '.' ~ NA_character_,
      BikeAlcDrg == 'Missing' ~ NA_character_,
      BikeAlcDrg == 'Unknown' ~ NA_character_,
      TRUE ~ BikeAlcDrg
    ),
    BikeAlcFlg = case_when(
      BikeAlcFlg == 'Missing' ~ NA_character_,
      BikeAlcFlg == 'Unknown' ~ NA_character_,
      TRUE ~ BikeAlcFlg
    ),
    BikeAlcFlg = fct_relevel(BikeAlcFlg, 'No', 'Yes'),
    BikeDir = if_else(BikeDir == 'Unknown', NA_character_, BikeDir),
    BikePos = if_else(BikePos == 'Unknown', NA_character_, BikePos),
    BikeRace = if_else(BikeRace == 'Unknown/Missing', NA_character_, BikeRace),
    BikeSex = if_else(BikeSex == 'Unknown', NA_character_, BikeSex),
    BikeSex = fct_relevel(BikeSex, 'Male', 'Female'),
    CrashAlcoh = fct_relevel(CrashAlcoh, 'No', 'Yes'),
    CrashGrp = if_else(
      CrashGrp == 'Other / Unknown - Insufficient Details', 
      NA_character_, 
      CrashGrp
    ),
    CrashLoc = if_else(CrashLoc == 'Unknown Location', NA_character_, CrashLoc),
    CrashDay = factor(CrashDay,
      levels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 
                 'Friday', 'Saturday')
    ),
    CrashMonth = factor(CrashMonth,
      levels = c('January', 'February', 'March' ,'April', 'May', 
                 'June', 'July', 'August', 'September', 'October', 
                 'November', 'December')
    ),
    CrashType = if_else(
      CrashType %in% c('Unknown Approach Paths', 'Unknown Location'), 
      NA_character_, 
      CrashType
    ),
    CrashType = str_replace_all(CrashType, '\\s|-|/', '.'),
    CrashType = str_replace_all(CrashType, '\\.{2,}', '.'),
    DrvrAge = as.numeric(DrvrAge),
    # unlikely anyone over 100 years old
    DrvrAge = if_else(
      DrvrAge > 100 | DrvrAge == '70+' | DrvrAge == 'Unknown', 
      NA_real_, 
      DrvrAge
    ),
    DrvrAgeGrp = if_else(
      DrvrAgeGrp == 'Unknown', 
      NA_character_, 
      DrvrAgeGrp
    ),
    DrvrAlcDrg = case_when(
      DrvrAlcDrg == '.' ~ NA_character_,
      DrvrAlcDrg == 'Missing' ~ NA_character_,
      DrvrAlcDrg == 'Unknown' ~ NA_character_,
      TRUE ~ DrvrAlcDrg
    ),
    DrvrAlcFlg = case_when(
      DrvrAlcFlg == 'Missing' ~ NA_character_,
      DrvrAlcFlg == 'Unknown' ~ NA_character_,
      TRUE ~ DrvrAlcFlg
    ),
    DrvrAlcFlg = fct_relevel(DrvrAlcFlg, 'No', 'Yes'),
    DrvrRace = if_else(
      DrvrRace == 'Unknown/Missing', 
      NA_character_, 
      DrvrRace
    ),
    DrvrSex = if_else(DrvrSex == 'Unknown', NA_character_, DrvrSex),
    DrvrSex = fct_relevel(DrvrSex, 'Male', 'Female'),
    HitRun = fct_relevel(HitRun, 'No', 'Yes'),
    LightCond = if_else(LightCond == 'Unknown', NA_character_, LightCond),
    NumBicsAin = if_else(NumBicsAin == '.', NA_character_, NumBicsAin),
    NumBicsBin = if_else(NumBicsBin == '.', NA_character_, NumBicsBin),
    NumBicsCin = if_else(NumBicsCin == '.', NA_character_, NumBicsCin),
    NumBicsKil = if_else(NumBicsKil == '.', NA_character_, NumBicsKil),
    NumBicsNoi = if_else(NumBicsNoi == '.', NA_character_, NumBicsNoi),
    NumBicsUin = if_else(NumBicsUin == '.', NA_character_, NumBicsUin),
    NumBicsTot = if_else(NumBicsTot == '.', NA_character_, NumBicsTot),
    NumLanes = if_else(NumLanes == 'Unknown', NA_character_, NumLanes),
    NumLanes = if_else(NumLanes == '9 or more lanes', '9+ lanes', NumLanes),
    NumLanes = factor(NumLanes, 
      levels = c('1 lane', '2 lanes', '3 lanes', '4 lanes', '5 lanes', 
                 '6 lanes', '7 lanes', '8 lanes', '9+ lanes')
    ),
    RdCharacte = if_else(RdCharacte == 'Unknown', NA_character_, RdCharacte),
    RdClass = if_else(RdClass %in% c('.', 'missing', 'Unknown'), NA_character_, RdClass),
    RdConditio = if_else(RdConditio == 'Unknown', NA_character_, RdConditio),
    RdConfig = if_else(RdConfig == 'Unknown', NA_character_, RdConfig),
    RdDefects = if_else(RdDefects %in% c('Unknown', 'Missing'), NA_character_, RdDefects),
    RdFeature = if_else(RdFeature == 'Missing', NA_character_, RdFeature),
    RdSurface = if_else(RdSurface %in% c('Unknown', 'Missing'), NA_character_, RdSurface),
    RuralUrban = if_else(RuralUrban == '.', NA_character_, RuralUrban),
    SpeedLimit = if_else(SpeedLimit == 'Unknown', NA_character_, SpeedLimit),
    # extra spaces between meaure and units have been removed
    SpeedLimit = str_replace(SpeedLimit, '\\s{2}', ' '), 
    SpeedLimit = factor(SpeedLimit,
      levels = c('5 - 15 MPH', '20 - 25 MPH', '30 - 35 MPH', '40 - 45 MPH', 
                 '50 - 55 MPH', '60 - 75 MPH')
    ),
    TraffCntrl = if_else(TraffCntrl == 'Missing', NA_character_, TraffCntrl),
    Workzone = fct_relevel(Workzone, 'No', 'Yes')
  ) %>%
  # transform char columns that need to be split
  separate(
    col = BikeInjury,
    into = c('BikeInjuryCat', 'BikeInjuryDisc'),
    sep = ': '
  ) %>%
  mutate(
    BikeInjuryCat = if_else(BikeInjuryCat == 'Unknown Injury', 'U', BikeInjuryCat),
    BikeInjuryDisc = if_else(is.na(BikeInjuryDisc), 'Unknown Injury', BikeInjuryDisc),
    # create dichotomous response variable in case multinomial isn't fruitful
    BikeInjurySerious = case_when(
      BikeInjuryCat %in% c('A' ,'K') ~ 1, 
      BikeInjuryCat %in% c('B', 'C', 'O') ~ 0,
      BikeInjuryCat %in% c('U') ~ NA_real_
    )
  ) %>%
  filter(
    !is.na(BikeInjurySerious)
  )
```

### Missing data

The amount of data missing throughout the dataset are noticeable and should imputed
if possible.  NumBics columns have the most missing data with 54.9%  missing.
It's unlikely that NumBics will be able to be imputed since majority values are
missing.

```{r}
# percent of missing data in each column
bike_crashes_clean %>%
  map(~ round(sum(is.na(.x)) / nrow(bike_crashes_clean) * 100, 1)) %>%
  map_df(~ if(.x > 0) .x) %>%
  mutate(tmp = NA) %>%
  pivot_longer(-tmp, values_to = '% missing', names_to = 'Variable') %>%
  select(-tmp) %>%
  kable(caption = 'Variables with missing values')
```

### Median imputation of biker age and driver age

```{r}
######################
# Median Imputations #
######################
bike_crashes_clean <- bike_crashes_clean %>%
  # group specific calculations and imputations
  group_by(BikeAgeGrp) %>%
  mutate(
    # median imputation for age group 70+
    BikeAge = if_else(is.na(BikeAge), median(BikeAge, na.rm = T), BikeAge)
  ) %>%
  ungroup() %>%
  # if no age group given for biker, just use median value of full dataset
  mutate(
    BikeAge = if_else(is.na(BikeAge), median(BikeAge, na.rm = T), BikeAge)
  ) %>%
  group_by(DrvrAgeGrp) %>%
  mutate(
    # median imputation for age (years) recorded as 70+
    DrvrAge = if_else(is.na(DrvrAge), median(DrvrAge, na.rm = T), DrvrAge)
  ) %>%
  ungroup() %>%
  # if no age group given for driver, just use median value of full dataset
  mutate(
    DrvrAge = if_else(is.na(DrvrAge), median(DrvrAge, na.rm = T), DrvrAge)
  ) %>%
  # convert all character vectors to factors
  mutate(across(where(is.character), ~ factor(.x))) %>%
  # find and remove any factor variables with less than 2 factors
  select(-where(~is.factor(.x) & length(levels(.x)) < 2)) %>% 
  # fill in NA's with No's if a Yes/No variable to preserve data
  mutate(across(where(
    ~is.factor(.x) & 'No' %in% levels(.x)), 
    ~replace_na(.x, 'No')
  ))
```

### Exploratory Data Analysis

#### Response Variable: Serious Bike Injury

Let's take a look at a geographical map of the serious crashes in the
data set.  The geographical data is sourced from R's maps package.  As it can 
be seen from the map below, a majority of the crashes seem to be occurring in
counties where large cities are located.

```{r explore_bike_crashes, echo=TRUE, message=FALSE, warning=FALSE}

county_bike_crashes <- bike_crashes_clean %>%
  filter(BikeInjurySerious == 1) %>%
  group_by(County) %>%
  summarize(
    crashes = n_distinct(CrashID)
  )

NC_map_coords <- map_data("county", "North Carolina") %>% 
  transmute(
    lon = long, 
    lat, 
    group, 
    county = str_to_title(subregion)
  )

NC_city_coords <- maps::us.cities %>% 
  filter(country.etc == 'NC') %>%
  transmute(
    name,
    lon = long,
    lat,
    pop,
    pop_pct = pop / sum(pop)
  )

NC_bike_crash_map_coords <- NC_map_coords %>%
  left_join(
    county_bike_crashes, 
    by = c('county' = 'County')
  ) %>% 
  group_by(county) %>%
  mutate(
    log_crashes = log(crashes),
    log_crashes = if_else(log_crashes == -Inf, 0, log_crashes)
  )

ggplot(mapping = aes(lon, lat)) +
  geom_polygon(
    mapping = aes(group = county, fill = crashes),
    col = 'grey',
    data = NC_bike_crash_map_coords
  ) +
  geom_point( 
    aes(size = pop_pct),
    color = 'black',
    alpha = .5,
    data = NC_city_coords
  ) +
  scale_fill_continuous(
    name = 'County serious\ninjury count',
    type = "viridis"
  ) +
  scale_size_area(name = 'City pop percent') +
  coord_quickmap() +
  ggtitle(
    'North Carolina: serious Injuries by county',
    subtitle = 'with city populations'
  )
```

The table below confirms that around 50% of the bike accidents with serious
injury occur in urban areas (>70% developed), followed by 32.5% of serious
injuries being in rural areas (<30% developed).

```{r}
bike_crashes_clean %>% 
  count(Locality, BikeInjurySerious) %>%
  mutate(
    BikeInjurySerious = if_else(BikeInjurySerious == 1, 'Yes', 'No'),
    pct = n / sum(n) * 100
  ) %>%
  kable(caption = 'Bike injury by locality', digits = 2)
```

### Explore categorical variables

```{r}
bike_crashes %>% 
  mutate(across(where(is.character), factor)) %>%
  select(where(~length(levels(.x)) <= 12)) %>%
  select(-matches('Age'), -matches('Alc'), -matches('Injury')) %>%
  map(summary) %>%
  map(names) %>%
  map(~ .x[.x != '.'])
bike_crashes %>% 
  mutate(across(where(is.character), factor)) %>%
  select(where(~length(levels(.x)) > 12)) %>%
  select(-matches('Age'), -matches('Alc'), -matches('Injury')) %>%
  map(summary) %>%
  map(names) %>%
  map(~ .x[.x != '.'])
```


### Explore numerical variables

```{r}
```

# Pre-Processing and Model Setup

Some variables will need to be converted to a numerical datatype for the
machine learning models to work properly. Categorical character columns
will need to be converted to factors. Dummy variables will be created
for every column that is not numerical. This will likely increase the
number of features in the dataset to the scale of hundreds of features.

Feature reduction will be carried out to reduce memory usage and computation 
time of fitting models. Feature reduction will be carried out by removing 
features are highly correlated or that have little to no variability and will 
thus not contribute much information to the models being used.  

## Create Dummy Variables

convert categorical variables to dummy variables. Some name fixes will be 
required to make column names more consistent.If any binary features have NA's
then replace with 0's to preserve largest complete dataset possible.

```{r create_dummies}
bike_crash_dummies_mdl <- dummyVars(
  # keep BikeInjurySerious in the matrix since already numeric
  ~ ., 
  data = bike_crashes_clean,
  sep = '.',
  fullRank = TRUE
)

bike_crash_dummies <- predict(
  bike_crash_dummies_mdl, 
  newdata = bike_crashes_clean
)

# clean up variable names
bike_crash_dummies <- bike_crash_dummies %>% 
  as_tibble() %>%
  rename_with(
    function(x) {
      tmp <- str_replace_all(x, '\\s|/|-', '.')
      tmp <- str_replace_all(tmp, '\\(|\\)|\\+|\\,', '')
      tmp <- str_replace_all(tmp, '>', 'GT')
      tmp <- str_replace_all(tmp, '<', 'LT')
      tmp <- str_replace_all(tmp, '%', 'pct')
      tmp <- str_replace_all(tmp, '\\.{2,}', '.')
    }
  )

# fill in NA's for binary variables with zero since there was no detection
bike_crash_dummies <- bike_crash_dummies %>% 
  mutate(across(
    where(~n_distinct(.x) == 3 & any(is.na(unique(.x)))), 
    ~replace_na(.x, 0)
  ))

# bike_crash_dummies %>% glimpse()
```

## Correlated Variables

In this section, correlated features will be removed based on complete cases.
Any variables with zero variance after filtering for complete cases will be
removed because they won't offer any information to a model.

```{r corr_vars}
# if we look at correlation we'll have to use complete cases
# let's see what variables have the most missing data
# since approximately 13-14% are missing. With certain models complete.obs 
# should be sufficient.

# after filtering for complete cases, how many vars would have zero variance?
zero_variance_vars <- bike_crash_dummies %>%
  filter(complete.cases(.)) %>%
  nearZeroVar(., saveMetrics=TRUE) %>%
  filter(zeroVar) %>% 
  rownames()

# remove variables that are zero-variance after removing rows w/ missing data
bike_crash_dummies <- 
  bike_crash_dummies %>%
  select(-zero_variance_vars)

# Check for any correlated predictors
# compute correlation
bike_crash_dummies_cor <- cor(
  bike_crash_dummies %>%
    select(-BikeInjurySerious), # don't check the response variable
  use = 'complete.obs'
)

# find variables with high pair-wise correlation to potentially remove
correlated_vars <- findCorrelation(
  bike_crash_dummies_cor, 
  cutoff = .75, # default (.90)
  names = TRUE
)

# remove highly correlated variables of interest and zero var. variables
bike_crash_dummies <- bike_crash_dummies %>%
  select(
    -correlated_vars
  )
```

## Variables that are directly related to response

The following variables are directly related to the response and if used in a 
model, the results would be overly optimistic.

*   Ambulance - if ambulance used then it would already known to be serious.
*   BikeInjuryDisc - directly related to response
*   BikeInjuryCat - directly related to response
*   NumBicsX - all variables are related to the BikeInjuryCat

```{r}
bike_crash_dummies <- bike_crash_dummies %>%
  select(
    -matches('AmbulanceR'),
    -matches('Disc'),
    -matches('Cat'),
    -(NumBicsAin.1:NumBicsNoi.2)
  )
```

## Save Datasets

At this point dataset have been cleaned as much as possible. Save datasets.

```{r save_derived_data}
saveRDS(bike_crashes_clean, './derived_data/bike_crashes_clean.rds')
saveRDS(bike_crash_dummies, './derived_data/bike_crash_dummies.rds')
# remove everything from the environment to releive memory
rm(list = ls())
```

# Model Development

## Create train and test dataset partitions

training dataset accounts for 80% of the full dataset, while testing dataset 
will account for 20%.

```{r}
loadRDS('./derived_data/bike_crash_dummies.rds')
set.seed(24)

train_index <- createDataPartition(
  bike_crash_dummies$BikeInjurySerious, 
  p = .8, 
  list=FALSE
)
train_dat <- bike_crash_dummies[train_index,]
test_dat <- bike_crash_dummies[-train_index,]

train_dat %>%
  count(BikeInjurySerious) %>%
  mutate(pct = n / sum(n) * 100) %>%
  kable(caption = 'Train data: response class summary')

test_dat %>%
  count(BikeInjurySerious) %>%
  mutate(pct = n / sum(n) * 100) %>%
  kable(caption = 'Test data: response class summary')
```

## Remove near-zero variance features

Remove variables that have near-zero variance since they will likely not
contribute much information to the classification models. Any variable that
has less than 5% distinct values out of total number of samples will be removed. 

```{r near_zero_variance}
preproc_mdl <- preProcess(
  train_dat %>%
    select(-BikeInjurySerious) %>% 
    as.data.frame,
  method = c(
    'nzv'
  ),
  uniqueCut = 5 # % cutoff
)
# preproc_mdl$method

# run the preProcess predictions to create new dataset
bike_crashes_preProcessed <- predict(
  preproc_mdl,
  train_dat %>% as.data.frame
)

# convert the BikeInjurySerious variable from numeric to a factor that is
# variable name friendly

# train dataset
bike_crashes_preProcessed <-
  bike_crashes_preProcessed %>%
    mutate(
      BikeInjurySerious = factor(
        if_else(BikeInjurySerious == 1, 'Serious', 'Non.Serious'), 
        levels = c('Serious', 'Non.Serious')
      )
    )

# test dataset
test_dat <-
  test_dat %>%
    mutate(
      BikeInjurySerious = factor(
        if_else(BikeInjurySerious == 1, 'Serious', 'Non.Serious'), 
        levels = c('Serious', 'Non.Serious')
      )
    )
```

# Modeling

Multiple models will be evaluated for performance based on ROC metric
and by computation time. The best models will be used in an ensemble
model. Models will be chosen for the ensemble based on computation time,
ROC, complimentary results, i.e., one model may have higher sensitivity
while another may have higher specificity, and an ensemble could balance
these models.

The model predictors are selected based on caret's underlying training
methodology. All final model's hyper-parameters and predictors will be
chosen based on the optimum area under the ROC (i.e. AUC). AUC is a
performance metric that will balance sensitivity (true positive rate)
and specificity (true negative rate). AUC values range from 0 to 1 and a
value of 1 considered a perfect classification model. hyper-parameter
grid search is set to default which chooses hyper-parameters randomly.

Repeated k-fold cross-validation will carried out on all models below.
In some cases, the computation method is quite complex and is very slow
to compute (e.g. random forests). In these cases, repeated
cross-validation will be reduced to non-repeated cross-validation and/or
the k in k-fold will be reduced.

Since the prevalence of serious injury is so low (around 5%),
up-sampling has been used to increase representation when k-fold cross
validation occurs.

All models will be run on the training dataset.

## Linear Classifiers

Linear classifier models parameterize the probability of a response,
serious injury in our case, based on a linear combination data features.
These models are nice since we can intuitively see the importance of
each feature in the final model with respect to how they influence the
probability of a serious injury. In this section, a general linear model
w/ a binary link function (aka logistic classification model) will be
used which takes numerical features as inputs and will be used to
predict probability of having a serious injury. Principal components
analysis (PCA) will be attempted to see if the feature space can be
further reduced and model improved.

glmnet package is used for logistic regression with elastic-net penalty
built-in. PCA is carried out on second logistic model to see if there is
any improvement. Multiple hyper-parameters, alpha (mixing parameter: 0 =
ridge, 1 = lasso) and lambda (penalty size), will be explored by caret.

### Logistic (elastic-net) Model

```{r logistic_model}
set.seed(42)
# seeds to be used reproducable resampling
seeds <- vector(mode = "list", length = 31)
# longest set of hyper-params is 33 (logistic mdl)
for(i in 1:30) seeds[[i]] <- sample.int(1000, 33)
seeds[[31]] <- sample.int(1000, 1)

# caret control
binomial_control <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = 'final',
  sampling = 'up',
  method = 'repeatedcv',
  number = 10,
  repeats = 3,
  index = createMultiFolds(
    bike_crashes_preProcessed$BikeInjurySerious, 
    k = 10,
    times = 3
  ),
  seeds = seeds
)

######################################
# ElasticNet Logistic Classification #
######################################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)

log_mdl <- train(
  BikeInjurySerious ~ .,
  data = bike_crashes_preProcessed,
  method = 'glmnet',
  family = 'binomial',
  na.action = na.omit,
  trControl = binomial_control,
  tuneGrid = expand.grid(
    .alpha = seq(0, 1, .1),
    .lambda = c(0.1, 0.01, 0.001)
  ),
  seeds = seeds,
)
stopCluster(cl)
b <- Sys.time()
log_time <- b - a
print(log_time)
```

### Logistic Model (elastic-net) with PCA

```{r logistic_PCA_model}
set.seed(42)

#################################################################
# Elasticnet Logistic Classification using Principal Components #
#################################################################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)
log_PCA_mdl <- train(
  BikeInjurySerious ~ .,
  data = bike_crashes_preProcessed,
  method = 'glmnet',
  family = 'binomial',
  preProcess = c('pca'),
  na.action = na.omit,
  trControl = binomial_control,
  tuneGrid = expand.grid(
    .alpha = seq(0, 1, .1),
    .lambda = c(0.1, 0.01, 0.001)
  ),
  seeds = seeds
)
stopCluster(cl)
b <- Sys.time()
log_PCA_time <- b - a
print(log_PCA_time)
```

## Non-linear Classification Models

In some cases, linear classifiers do not perform well when the response
classes (i.e. serious, non-serious) do not have a linear relationship
with the features. Put another way, linear classifiers can not produce a
straight line in the feature space that separates the response classes.
Below, naive Bayes classifier, neural network and random forest models
are used. It would not be a good use of space to go into details of each
of these models. R has plenty of documentation on each model if one is
curious.

### Naive Bayes Model

```{r naive_bayes}
set.seed(42)

###############
# Naive Bayes #
###############
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)
nb_mdl <- train(
  BikeInjurySerious ~ .,
  data = bike_crashes_preProcessed,
  method = 'naive_bayes',
  na.action = na.omit,
  trControl = binomial_control,
  tuneGrid = expand.grid(
    .laplace = 0:1,
    .adjust = 0:1,
    .usekernel = c(T, F)
  )
)
stopCluster(cl)
b <- Sys.time()
nb_time <- b - a
print(nb_time)
```

### K-nearest Neighbors Model

```{r knn}
set.seed(42)

##################
# Neural Network #
##################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)
knn_mdl <- train(
  BikeInjurySerious ~ .,
  data = bike_crashes_preProcessed,
  method = 'knn',
  na.action = na.omit,
  trControl = binomial_control,
  tuneGrid = expand.grid(
    k = 2:21
  )
)
stopCluster(cl)
b <- Sys.time()
knn_time <- b - a
print(knn_time)
```

### Random Forests Model

```{r random_forests}
set.seed(42)

# random forests is the most computation intensive with repeatedcv
# will reduce to non-repeated cv since random forests has methods
# that are similar to cross-validation already (many trees in a forest)
rf_control <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = 'final',
  sampling = 'up',
  method = 'cv',
  number = 10,
  index = createFolds(
    bike_crashes_preProcessed$BikeInjurySerious, 
    k = 10,
    returnTrain = TRUE
  ),
  # collect the first 5 seed lists and the final model seed 
  seeds = seeds[c(1:10, 31)] 
)

rf_tune_grid <- expand.grid(
  .mtry = 2:11,
  .splitrule = c('gini'),
  .min.node.size = 1
)

#################
# Random Forest #
#################
# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)
rf_mdl <- train(
  BikeInjurySerious ~ .,
  data = bike_crashes_preProcessed,
  method = 'ranger',
  num.trees = 100,
  importance = 'permutation',
  na.action = na.omit,
  trControl = rf_control,
  tuneGrid = rf_tune_grid
)
stopCluster(cl)
b <- Sys.time()
rf_time <- b - a
print(rf_time)
```

# Prediction Performance

In this section, each model will be evaluated on its' performance in
accuracy, sensitivity and specificity based on the optimum probability
cutoff used to classify each observation as a serious or non-serious
injury. You will see plots below that show that a model with high
accuracy is not necessarily the best at classifying observations. In
fact, a model that predicts non-serious 100% of the time has a model
accuracy of 95% since the prevalence of serious injury is 5% in the
dataset.

All final predictions will be based on test dataset.

```{r pred_perf_fn, message=FALSE, warning=FALSE, include=FALSE}
# performance function
mdl_train_performance <- function(mdl, conf_matrix = FALSE, ensemble = FALSE, mdl_lst = NULL) {
  set.seed(42)
  # train data
  mdl_train_dat <- bike_crashes_preProcessed
  
  # model variable importance
  variable_imp <- varImp(mdl)
  if(ensemble) {
    variable_imp <- variable_imp
  } else {
    variable_imp <- variable_imp$importance
  }
  
  # construct test dataset based on model type
  mdl_predictors <- NULL
  if(ensemble) {
    mdl_predictors <- map(mdl_lst, predictors) %>% 
      flatten() %>% 
      unlist() %>% 
      unique()
  } else {
    mdl_predictors <- attr(formula(mdl), 'term.labels')
  }
  mdl_train_dat <- mdl_train_dat[
    complete.cases(mdl_train_dat[, names(mdl_train_dat) %in% mdl_predictors]),
  ]
  
  # predictions based on training dataset
  mdl_train_pred <- predict(mdl, newdata = mdl_train_dat)
  
  # performance based on train data
  train_conf_mat <- NULL
  if(conf_matrix) {
    train_conf_mat <- confusionMatrix(
      factor(mdl_train_pred, levels = c('Serious', 'Non.Serious')),
      factor(mdl_train_dat$BikeInjurySerious, levels = c('Serious', 'Non.Serious'))
    )
  }
  
  mdl_train_pred_prob <- predict(mdl, newdata = mdl_train_dat, type = 'prob')
  if(ensemble) {
    mdl_train_pred_prob <- tibble(
      Serious = mdl_train_pred_prob
    ) %>%
    mutate(
      Non.Serious = 1 - Serious
    ) 
  }
  
  perf <- tibble(
      p = seq(.01, 0.99, by = .01),
      mdl_prob = list(mdl_train_pred_prob),
      obs = list(mdl_train_dat$BikeInjurySerious)
    ) %>%
    mutate(
      pred = map2(
        p, 
        mdl_prob,
        function(p, probs) {
          if_else(probs$Serious > p, 'Serious', 'Non.Serious')
        }
      )
    ) %>% 
    unnest(c(obs, pred)) %>%
    group_by(p) %>%
    summarize(
      n_TP = sum(obs == 'Serious' & pred == obs),
      n_obs_P = sum(obs == 'Serious'),
      n_TN = sum(obs == 'Non.Serious' & pred == obs),
      n_obs_N = sum(obs == 'Non.Serious'),
      accuracy = (n_TP + n_TN) / (n_obs_P + n_obs_N),
      sensitivity = n_TP / n_obs_P,
      specificity = n_TN / n_obs_N
    )
  
  perf_plot <- perf %>%
    ggplot(data = .) +
    geom_line(aes(x = p, y = sensitivity, color = 'sensitivity'), size = 1) +
    geom_line(aes(x = p, y = specificity, color = 'specificity'), size = 1) +
    geom_line(aes(x = p, y = accuracy, color = 'accuracy'), size = 1) +
    xlab('p (percent cutoff)') +
    ylab('performance rate') +
    scale_color_manual(
      name = 'Performance Measures', 
      values = c('sensitivity' = 'red', 'specificity' = 'dark green', 'accuracy' = 'blue'),
      ) +
    scale_x_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = seq(0, 1, by = 0.05)) +
    scale_y_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = seq(0, 1, by = 0.05))
  
  return(
    list(
      train_conf_matrix = train_conf_mat,
      variable_importance = variable_imp,
      train_performance_plot = perf_plot
    )
  )
}

mdl_test_performance <- function(mdl, prob_cutoff = NULL, ensemble = FALSE, mdl_lst = NULL) {
  set.seed(42)
  # construct test dataset based on model type
  mdl_predictors <- NULL
  mdl_test_dat <- NULL
  if(ensemble) {
    mdl_predictors <- map(mdl_lst, predictors) %>% 
      flatten() %>% 
      unlist() %>% 
      unique()
  } else {
    mdl_predictors <- attr(formula(mdl), 'term.labels')
  }
  mdl_test_dat <- test_dat[
    complete.cases(test_dat[, names(test_dat) %in% mdl_predictors]),
  ]
  
  # predictions based on training probability cutoff and test dataset
  mdl_test_pred_prob <- predict(mdl, newdata = test_dat, type = 'prob')
  if(!ensemble) mdl_test_pred_prob <- mdl_test_pred_prob$Serious

  perf <- tibble(
      obs = mdl_test_dat$BikeInjurySerious,
      prob = mdl_test_pred_prob
    ) %>%
    mutate(
      pred = if_else(prob > prob_cutoff, 'Serious', 'Non.Serious')
    ) %>%
    unnest(c(obs, pred)) %>%
    summarize(
      n_TP = sum(obs == 'Serious' & pred == obs),
      n_obs_P = sum(obs == 'Serious'),
      n_TN = sum(obs == 'Non.Serious' & pred == obs),
      n_obs_N = sum(obs == 'Non.Serious'),
      accuracy = (n_TP + n_TN) / (n_obs_P + n_obs_N),
      sensitivity = n_TP / n_obs_P,
      specificity = n_TN / n_obs_N
    )
  
  return(perf)
}
```

## Linear Predictions

Both standard logistic model and PCA logistic model have similar
outcomes. Logistic model without PCA was faster at achieving a similar
result. Therefore PCA was not useful in this case and will not be used
elsewhere.

### Logistic (elastic net) Predictions

```{r linear_preds}
# Logistic Classification
log_train_perf <- mdl_train_performance(log_mdl, conf_matrix = TRUE)
# customize output
log_train_perf$train_conf_matrix
log_train_perf$variable_importance %>%
  arrange(desc(Overall)) %>%
  slice(1:20) %>%
  kable(caption = "Top 20 most import features", digits = 4)
log_train_perf$train_performance_plot
log_mdl_perf <- mdl_test_performance(log_mdl, prob_cutoff = .4517) %>% 
  mutate(model = 'logistic')
log_mdl_perf %>% 
  kable(caption = 'Test Data Performance: GLM')
```

### Logistic (elastic net) with PCA Predictions

PCA does not seem to improve the logistic model enough to be useful.

```{r}
# Logistic Classification with PCA
log_PCA_train_perf <- mdl_train_performance(log_PCA_mdl, conf_matrix = TRUE)
# customize output
log_PCA_train_perf$train_conf_matrix
log_PCA_train_perf$variable_importance %>%
  arrange(desc(Overall)) %>%
  slice(1:20) %>%
  kable(caption = "Top 20 most import features", digits = 4)
log_PCA_train_perf$train_performance_plot
log_PCA_perf <- mdl_test_performance(log_PCA_mdl, prob_cutoff = .465) %>%
  mutate(model = 'logistic w/ PCA')
log_PCA_perf %>%
  kable(caption = 'Test Data Performance: GLM w/ PCA')
```

## Non-linear Predictions Predictions

Naive bayes classifier is the fastest and has a very similar result the
GLM models above. Both neural network and random forest take
considerably longer to compute and don't perform much better than naive
bayes. Therefore, only the naive bayes model made it into ensemble
model.

Notice that for random forest model, since we dropped the repeated part
of the cross-validation and dropped the k in k-fold to 5 from 10, the
model is over-fitting a bit more than the other models. The high
accuracy values in the random forest performance plot and the low
accuracy with the test dataset confirm this. Since the random forest
algorithm is slow compared to these other non-linear models it was not
be used in ensemble model.

### Naive Bayes Predictions

```{r naive_bayes_pred}
# Naive Bayes
nb_train_perf <- mdl_train_performance(nb_mdl, conf_matrix = TRUE)
# customize output
nb_train_perf$train_conf_matrix
nb_train_perf$variable_importance %>%
  transmute(Importance = Serious) %>%
  arrange(desc(Importance)) %>%
  slice(1:20) %>%
  kable(caption = "Top 20 most import features", digits = 4)
nb_train_perf$train_performance_plot
nb_mdl_perf <- mdl_test_performance(nb_mdl, prob_cutoff = .265) %>% 
  mutate(model = 'naive bayes')
nb_mdl_perf %>% 
  kable(caption = 'Test Data Performance: Naive Bayes')
```

### K-nearest Neighbors Predictions

Predictions for K-nearest neighbor take a lot longer to compute than
other models.

```{r knn_pred}
# K-nearest Neighbors
knn_train_perf <- mdl_train_performance(knn_mdl, conf_matrix = TRUE)
knn_train_perf$train_conf_matrix
knn_train_perf$variable_importance %>%
  transmute(Importance = Serious) %>%
  arrange(desc(Importance)) %>%
  slice(1:20) %>%
  kable(caption = '"Top 20 most import features"', digits = 4)
knn_train_perf$train_performance_plot

knn_mdl_perf <- mdl_test_performance(knn_mdl, prob_cutoff = .4) %>%
  mutate(model = 'K-nearest neighbors')
knn_mdl_perf %>%
  kable(caption = 'Test Data Performance: K-nearest Neighbor')
```

### Random Forest Predictions

```{r rf_pred}
# Random Forest
rf_train_perf <- mdl_train_performance(rf_mdl, conf_matrix = TRUE)
rf_train_perf$train_conf_matrix
rf_train_perf$variable_importance %>%
  arrange(desc(Overall)) %>%
  slice(1:20) %>%
  kable(caption = '"Top 20 most import features"', digits = 4)
rf_mdl_perf <- mdl_test_performance(rf_mdl, prob_cutoff = .0945) %>% 
  mutate(model = 'random forest')
rf_mdl_perf %>%
  kable(caption = 'Test Data Performance: Random Forest')
```

# Model Performance on Test Data

The goal of this section to select the models that will go on to
ensemble model.

Logistic model, naive Bayes and random forests all perform similarly.
However, both logistic models, with/without PCA included in the
analysis, perform better than the others as far as optimizing
sensitivity while balancing accuracy and specificity. Of the two
logistic models, the one that includes the PCA with the model performs
slightly better. Since the time difference for each logistic model is
comparable, the PCA logistic model would seem like the most useful
model. However, PCA model results are difficult to understand because
they are linear combinations of other features. Therefore, the non-PCA
version of the model will be used in the ensemble model since the
performance isn't that different and is slightly faster. Also worth of
note, if PCA is used in one model and not in others, the ensemble model
is harder to interpret since it has mixed PCA features and raw feature
names listed in caret::varImp.

The naive Bayes model computes the fastest with the lower accuracy due
to having a low specificity. Considering how fast the computation is and
not having a huge drop in accuracy, this is still a reasonable model.
Naive Bayes will be included in the ensemble model since it could be
picking up on different features than the linear or other nonlinear
models.

K-nearest neighbors was the worst performer in accuracy due to the
lowest specificity and therefore, will not move on to the ensemble
model.

Random forest model did not perform as well as expected give how well it
performed on the training data. This seems to indicate that the model is
over fit which is the opposite of what we'd expect from an ensemble
decision tree count of 100 and 10-fold cross validation. It's possible
that the random forest model is picking up on something that the other
models aren't and will be included in the final ensemble model.

```{r model_summaries}
list(
  log_mdl_perf,
  log_PCA_perf,
  nb_mdl_perf,
  knn_mdl_perf,
  rf_mdl_perf
) %>% 
  reduce(
    bind_rows
  ) %>% 
  select(model, everything()) %>%
  left_join(
    tibble(
      model = c(
        'logistic', 
        'logistic w/ PCA', 
        'naive bayes', 
        'K-nearest neighbors', 
        'random forest'
      ),
      run_time = c(
        log_time, 
        log_PCA_time, 
        nb_time,
        knn_time,
        rf_time
      )
    )
  ) %>%
  kable(caption = 'Model Performance Comparison with Test Data')
```

# Ensemble Model

## Ensemble Classifier

Logistic, naive Bayes random forests will be used in an ensemble
learning model. Note that random forests took the longest to run before
and is the bottleneck of the ensemble model. However, since we've
already found the tuning of the models, grid search part of the fitting
is not needed and will speed up the computation time.

```{r ensemble_classifier}
set.seed(42)
# seeds to be used reproducable resampling
# since we have the hyper-parameters locked in,
# we can add additional cross-validation iterations
# in an attempt to raise test data peformance
ens_seeds <- vector(mode = "list", length = 51)
# longest set of hyper-params is 33 (logistic mdl)
for(i in 1:51) ens_seeds[[i]] <- sample.int(1000, 1)

ensemble_control <- trainControl(
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = 'final',
  sampling = 'up',
  method = 'repeatedcv',
  number = 10,
  repeats = 5,
  index = createMultiFolds(
    bike_crashes_preProcessed$BikeInjurySerious, 
    k = 10,
    times = 5
  ),
  # only need 1 seed per 51 CV instances
  seeds = ens_seeds
)

# timing process
a <- Sys.time()
# create clusters
cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)
model_list <- caretList(
  BikeInjurySerious ~ .,
  data = bike_crashes_preProcessed,
  tuneList = list(
    log_mdl = caretModelSpec(
      method = 'glmnet',
      family = 'binomial',
      tuneGrid = expand.grid(
        .alpha = 0.6,
        .lambda = 0.01
      )
    ),
    nb_mdl = caretModelSpec(
      method = 'naive_bayes',
      tuneGrid = expand.grid(
        .laplace = 1, 
        .adjust = 1,
        .usekernel = TRUE
      )
    ),
    rf_mdl = caretModelSpec(
      method = 'ranger', 
      num.tree = 100,
      importance = 'permutation',
      tuneGrid = expand.grid(
        .mtry = 8,
        .splitrule = 'gini',
        .min.node.size = 1
      )
    )
  ),
  trControl = ensemble_control
)
stopCluster(cl)

cl <- makeCluster(detectCores() / 2)
registerDoParallel(cl)
ensemble_mdl <- caretEnsemble(
  model_list, 
  metric = 'ROC',
  trControl = trainControl(
    summaryFunction = twoClassSummary,
    classProbs = TRUE
  )
)
stopCluster(cl)
b <- Sys.time()
ensemble_time <- b - a
print(ensemble_time)

# model_list
ensemble_mdl
# ggplot(resamples(model_list))
# modelCor(resamples(model_list))
```

## Ensemble Predictions

```{r ensemble_preds}
ensemble_train_perf <- mdl_train_performance(
  ensemble_mdl, 
  conf_matrix = TRUE, 
  ensemble = TRUE, 
  mdl_lst = model_list
)
# customize output
ensemble_train_perf$train_conf_matrix
ensemble_train_perf$variable_importance %>%
  arrange(desc(overall)) %>%
  slice(1:20) %>%
  kable(
    caption = "Top 20 most import features by each model and overall", 
    digits = 4
  )
ensemble_train_perf$train_performance_plot

ensemble_mdl_perf <- mdl_test_performance(
  ensemble_mdl, 
  prob_cutoff = .0666,
  ensemble = TRUE,  
  mdl_lst = model_list
) %>%
  mutate(model = 'ensemble (Log & NB & RF)')
ensemble_mdl_perf %>%
  kable(caption = 'Test Data Performance: Ensemble')
```

# Conclusions

All model performance metrics are listed below with the ensemble model
having the best performance. The ensemble model includes an elasticnet
logistic model, naive Bayes model and a random forests model. The
variable importance for all models including the overall ensemble model
importance weight per predictor is provided.

```{r ensemble_vs_other_mdls}
list(
  log_mdl_perf,
  log_PCA_perf,
  nb_mdl_perf,
  knn_mdl_perf,
  rf_mdl_perf,
  ensemble_mdl_perf
) %>% 
  reduce(
    bind_rows
  ) %>% 
  select(model, everything()) %>%
  left_join(
    tibble(
      model = c(
        'logistic', 
        'logistic w/ PCA', 
        'naive bayes', 
        'K-nearest neighbors', 
        'random forest',
        'ensemble (Log & NB & RF)'
      ),
      run_time = c(
        log_time, 
        log_PCA_time, 
        nb_time,
        knn_time,
        rf_time,
        ensemble_time
      )
    )
  ) %>%
  kable(caption = 'Model Performances Comparison (w/ Ensemble)')
```

With the ensemble model being selected as the best model, the top 20
most important variables are as follows:

```{r}
ensemble_train_perf$variable_importance %>%
  arrange(desc(overall)) %>%
  slice(1:20) %>%
  kable(
    caption = "Top 20 most import features by each model and overall", 
    digits = 4
  )
```

## Possible Interpretations of Predictors and Suggestions

The first 5 predictors by overall importance make sense as far as being
predictive of serious bike crashes in that high speed limits (50 - 55
mph), bicyclist under the influence of alcohol, bicyclist failing to
yield mid-block and bike position crosswalk/crossing/driveway all point
to the bicyclist not be a responsible commuter.

Notice, I did not mention daylight conditions being the second highest
predictor of serious bike accidents. This is because it is not really
clear why this is such a good predictor. It could be due to low
visibility or warning of a bicyclist and their intentions. If a
bicyclist is biking at night, they would most likely have bike lights on
which helps drivers to see that an unprotected traveler is in the road
and gives their direction since the bike light points forward and tail
light points backwards. In addition, during the day, there is more
traffic on the street and the sidewalk which could distract drivers.

It may seem surprising that straight roads is a good predictor serious
bike crashes, but if you think about when you drive on a straight road
for a while and you get into "the zone" where you don't remember the
last 5-10 minutes of driving because you were on autopilot. This could
be the case especially if the speeds are 40 - 55 mph where it would very
difficult to avoid an obstacle in the road.

I don't think we have enough data to explain why a driver's race is a
predictor of seriousness of a bike-vehicle crash. We would need to know
more about the state of North Carolina and how roads in predominantly
black neighborhoods differ in bike friendliness and traffic from roads
in predominantly white neighborhoods. We may find that there is an
uneven distribution of bike friendly roads in either neighborhood which
could affect a driver's experience how to share the road with a
bicyclist. No conclusions can be made with the data we have here. These
are just possible things we could look into with more socioecenomic
data.

Rural crashes are on the top 20 predictors list with development type:
farms, woods, pastures; non-city rural. The main take away from these
results is that bicyclists are not being protected on rural roads where
touring bicyclists spend most of their time. A recommendation that I
have is to cities that book-end the roads that are most heavily traveled
by bicyclists, invest in extending the shoulder of these roads and to
include bike signs. This could later be added to the bike crash data as
"bicycle improved rural road" and the treatment could be tested again
roads that were not treated (control).
